{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "import seaborn as sns\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "import seaborn as sns\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_df(data_dir='shords_dataset/'):\n",
    "    data = []\n",
    "    for cl in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, cl)\n",
    "        for file in os.listdir(class_dir):\n",
    "            if file.endswith('wav'):\n",
    "                file_path = os.path.join(class_dir, file)\n",
    "                data.append({'file_path': file_path, 'class': cl})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shords_dataset/major/4_21.wav</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shords_dataset/major/1_8.wav</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shords_dataset/major/9_49.wav</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shords_dataset/major/9_61.wav</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shords_dataset/major/9_75.wav</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file_path  class\n",
       "0  shords_dataset/major/4_21.wav  major\n",
       "1   shords_dataset/major/1_8.wav  major\n",
       "2  shords_dataset/major/9_49.wav  major\n",
       "3  shords_dataset/major/9_61.wav  major\n",
       "4  shords_dataset/major/9_75.wav  major"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data_to_df()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df = data[data['class'] == 'major']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([32, 1, 128, 400])\n",
      "Spectrogram shape: torch.Size([26, 1, 128, 400])\n"
     ]
    }
   ],
   "source": [
    "def load_and_convert_to_spectrogram(file_path, n_fft=2048, hop_length=512, n_mels=128, pad_mode='constant', pad_value=-80.0):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    spectrogram = librosa.feature.melspectrogram(y = audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)  # Convert to dB scale\n",
    "    \n",
    "    # Add padding to make all spectrograms the same size\n",
    "    max_length = 400  # Adjust this value as needed based on your data\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        pad_width = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, pad_width=((0, 0), (0, pad_width)), mode=pad_mode, constant_values=pad_value)\n",
    "    else:\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, n_fft=2048, hop_length=512, n_mels=128):\n",
    "        self.df = df\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df.iloc[idx]['file_path']\n",
    "        \n",
    "        # Load and convert audio to spectrogram\n",
    "        spectrogram = load_and_convert_to_spectrogram(file_path, self.n_fft, self.hop_length, self.n_mels)\n",
    "        \n",
    "        # Convert spectrogram to PyTorch tensor\n",
    "        spectrogram = torch.FloatTensor(spectrogram)  # Convert to float tensor\n",
    "        \n",
    "        # Add channel dimension (assuming you're using CNNs)\n",
    "        spectrogram = spectrogram.unsqueeze(0)  # Shape: (1, n_mels, T), where T is time steps\n",
    "        \n",
    "        return spectrogram\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = AudioDataset(major_df)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example of iterating through the data loader\n",
    "for batch in data_loader:\n",
    "    spectrogram = batch\n",
    "    print(f\"Spectrogram shape: {spectrogram.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Define Generator and Discriminator architectures\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_dim[0], 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, output_dim[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_dim[0], 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(nn.Module):\n",
    "    def __init__(self, G_AB, G_BA, D_A, D_B):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.G_AB = G_AB  # Generator: Domain A to B\n",
    "        self.G_BA = G_BA  # Generator: Domain B to A\n",
    "        self.D_A = D_A    # Discriminator: Domain A\n",
    "        self.D_B = D_B    # Discriminator: Domain B\n",
    "    \n",
    "    def forward(self, x_A, x_B):\n",
    "        # Forward pass through generators\n",
    "        x_BA = self.G_BA(x_B)  # Generate B from A\n",
    "        x_AB = self.G_AB(x_A)  # Generate A from B\n",
    "        \n",
    "        # Forward pass through discriminators (for adversarial loss)\n",
    "        pred_A = self.D_A(x_A)  # Discriminate real A\n",
    "        pred_BA = self.D_A(x_BA)  # Discriminate generated A (from B)\n",
    "        \n",
    "        pred_B = self.D_B(x_B)  # Discriminate real B\n",
    "        pred_AB = self.D_B(x_AB)  # Discriminate generated B (from A)\n",
    "        \n",
    "        return x_BA, x_AB, pred_A, pred_BA, pred_B, pred_AB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (13): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (16): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (19): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (22): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = (1, 128, 400)\n",
    "output_dim = (1, 128, 400)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()  # Mean Squared Error loss for GAN loss\n",
    "criterion_cycle = nn.L1Loss()  # L1 loss for cycle consistency\n",
    "\n",
    "# Initialize generators and discriminators\n",
    "G_AB = Generator(input_dim, output_dim)\n",
    "G_BA = Generator(input_dim, output_dim)\n",
    "D_A = Discriminator(input_dim)\n",
    "D_B = Discriminator(input_dim)\n",
    "\n",
    "# Initialize CycleGAN model\n",
    "cycle_gan = CycleGAN(G_AB, G_BA, D_A, D_B)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Print model summary (optional)\n",
    "print(G_AB)\n",
    "print(D_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[0;32m----> 6\u001b[0m         real_A \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m]  \u001b[39m# Assuming 'A' corresponds to real_A in your dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         real_B \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Assuming 'B' corresponds to real_B in your dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[39m# Move data to device if using GPU\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "device = 'mps'\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        real_A = batch['A']  # Assuming 'A' corresponds to real_A in your dataset\n",
    "        real_B = batch['B']  # Assuming 'B' corresponds to real_B in your dataset\n",
    "        \n",
    "        # Move data to device if using GPU\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "        \n",
    "        # Train Discriminator A\n",
    "        optimizer_D_A.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_B = cycle_gan.G_AB(real_A)\n",
    "        pred_real_A = cycle_gan.D_A(real_A)\n",
    "        pred_fake_B = cycle_gan.D_A(fake_B.detach())\n",
    "        \n",
    "        loss_D_A_real = criterion_GAN(pred_real_A, torch.ones_like(pred_real_A))\n",
    "        loss_D_A_fake = criterion_GAN(pred_fake_B, torch.zeros_like(pred_fake_B))\n",
    "        \n",
    "        loss_D_A = 0.5 * (loss_D_A_real + loss_D_A_fake)\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        # Train Discriminator B\n",
    "        optimizer_D_B.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_A = cycle_gan.G_BA(real_B)\n",
    "        pred_real_B = cycle_gan.D_B(real_B)\n",
    "        pred_fake_A = cycle_gan.D_B(fake_A.detach())\n",
    "        \n",
    "        loss_D_B_real = criterion_GAN(pred_real_B, torch.ones_like(pred_real_B))\n",
    "        loss_D_B_fake = criterion_GAN(pred_fake_A, torch.zeros_like(pred_fake_A))\n",
    "        \n",
    "        loss_D_B = 0.5 * (loss_D_B_real + loss_D_B_fake)\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Adversarial loss\n",
    "        pred_fake_A = cycle_gan.D_B(fake_A)\n",
    "        pred_fake_B = cycle_gan.D_A(fake_B)\n",
    "        \n",
    "        loss_GAN_AB = criterion_GAN(pred_fake_A, torch.ones_like(pred_fake_A))\n",
    "        loss_GAN_BA = criterion_GAN(pred_fake_B, torch.ones_like(pred_fake_B))\n",
    "        \n",
    "        # Cycle-consistency loss\n",
    "        recov_A = cycle_gan.G_BA(fake_B)\n",
    "        recov_B = cycle_gan.G_AB(fake_A)\n",
    "        \n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        \n",
    "        # Total generator loss\n",
    "        loss_G = loss_GAN_AB + loss_GAN_BA + 10 * (loss_cycle_A + loss_cycle_B)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Print losses\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], \"\n",
    "                  f\"Loss_G: {loss_G.item():.4f}, Loss_D_A: {loss_D_A.item():.4f}, Loss_D_B: {loss_D_B.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
